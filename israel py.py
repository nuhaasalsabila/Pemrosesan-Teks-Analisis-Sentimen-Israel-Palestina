# -*- coding: utf-8 -*-
"""Israel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J9I4K5P7WLT4an4lO6YsEGTfkHYSp7bE

### Import Library
"""

pip install Sastrawi

import nltk
from nltk.tokenize import word_tokenize

# Download the NLTK data for tokenization
nltk.download('punkt')

def tokenizingText(text):  # Tokenizing or splitting a string, text into a list of tokens
    text = word_tokenize(text)
    return text

import pandas as pd
pd.options.mode.chained_assignment = None
import numpy as np
seed = 0
np.random.seed(seed)
import matplotlib.pyplot as plt
import seaborn as sns

import datetime as dt
import re
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from wordcloud import WordCloud

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score

"""# Prepare untuk fitur yang digunakan

Tokenize
"""

import nltk
from nltk.tokenize import word_tokenize

nltk.download('stopwords')
def tokenizingText(text):
    text = word_tokenize(text)
    return text

import csv
input_file_path = '/content/israel no buzzer.csv'
output_file_path = 'rawel.csv'

try:
    with open(input_file_path, 'r') as input_file, open(output_file_path, 'w', newline='') as output_file:
        pembaca = csv.reader(input_file)
        penulis = csv.writer(output_file)
        header = next(pembaca)
        jumlah_kolom = len(header)
        penulis.writerow(["full_text"])

        for nomor_baris, baris in enumerate(pembaca, start=2):
            kolom_gabungan = ','.join(baris)
            penulis.writerow([kolom_gabungan])

    print(f"File CSV yang sudah diperbaiki disimpan di: {output_file_path}")

except Exception as e:
    print(f"Terjadi kesalahan: {e}")

tweets_data = pd.read_csv('/content/rawel.csv')
tweets = tweets_data[['full_text']]
tweets

import pandas as pd
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
import string

alay_dict = pd.read_csv('/content/singkatan-lib.csv', header=None, names=['awal', 'replace'], index_col='awal')

def replace_alay(text):
    for original, replacement in alay_dict.iterrows():
        text = re.sub(r'\b{}\b'.format(re.escape(original)), replacement['replace'], text)
    return text

def cleaningText(text):
    text = re.sub(r'@[A-Za-z0-9]+', '', text)
    text = re.sub(r'#[A-Za-z0-9]+', '', text)
    text = re.sub(r'RT[\s]', '', text)
    text = re.sub(r"http\S+", '', text)
    text = re.sub(r'[0-9]+', '', text)
    emoticons_pattern = (
        r'ðŸ˜Š|ðŸ˜€|ðŸ˜|ðŸ˜‚|ðŸ¤£|ðŸ˜ƒ|ðŸ˜„|ðŸ˜…|ðŸ˜†|ðŸ˜‡|ðŸ˜‰|ðŸ˜Š|ðŸ™‚|ðŸ™ƒ|ðŸ˜‹|ðŸ˜Œ|ðŸ˜|ðŸ˜Ž|ðŸ˜|ðŸ˜|ðŸ˜‘|ðŸ˜’|ðŸ˜“|ðŸ˜”|ðŸ˜•|ðŸ˜–|'
        r'ðŸ˜—|ðŸ˜˜|ðŸ˜™|ðŸ˜š|ðŸ˜›|ðŸ˜œ|ðŸ˜|ðŸ˜ž|ðŸ˜Ÿ|ðŸ˜ |ðŸ˜¡|ðŸ˜¢|ðŸ˜£|ðŸ˜¤|ðŸ˜¥|ðŸ˜¦|ðŸ˜§|ðŸ˜¨|ðŸ˜©|ðŸ˜ª|ðŸ˜«|ðŸ˜¬|ðŸ˜­|ðŸ˜®|ðŸ˜¯|ðŸ˜°|ðŸ˜±|ðŸ˜²|'
        r'ðŸ˜³|ðŸ˜´|ðŸ˜µ|ðŸ˜¶|ðŸ˜·|ðŸ˜¸|ðŸ˜¹|ðŸ˜º|ðŸ˜»|ðŸ˜¼|ðŸ˜½|ðŸ˜¾|ðŸ˜¿|ðŸ™€|ðŸ™|ðŸ™‚|ðŸ™ƒ|ðŸ™„|ðŸ™…|ðŸ™†|ðŸ™‡|ðŸ™ˆ|ðŸ™‰|ðŸ™Š|ðŸ™‹|ðŸ™Œ|ðŸ™|'
        r'ðŸ™Ž|ðŸ™|ðŸ˜|ðŸ˜‘|ðŸ˜’|ðŸ˜“|ðŸ˜”|ðŸ˜•|ðŸ˜–|ðŸ˜—|ðŸ˜˜|ðŸ˜™|ðŸ˜š|ðŸ˜›|ðŸ˜œ|ðŸ˜|ðŸ˜ž|ðŸ˜Ÿ|ðŸ˜ |ðŸ˜¡|ðŸ˜¢|ðŸ˜£|ðŸ˜¤|ðŸ˜¥|ðŸ˜¦|ðŸ˜§|ðŸ˜¨|ðŸ˜©|'
        r'ðŸ˜ª|ðŸ˜«|ðŸ˜¬|ðŸ˜­|ðŸ˜®|ðŸ˜¯|ðŸ˜°|ðŸ˜±|ðŸ˜²|ðŸ˜³|ðŸ˜´|ðŸ˜µ|ðŸ˜¶|ðŸ˜·|ðŸ˜¸|ðŸ˜¹|ðŸ˜º|ðŸ˜»|ðŸ˜¼|ðŸ˜½|ðŸ˜¾|ðŸ˜¿|ðŸ™€|ðŸ™|ðŸ™‚|ðŸ™ƒ|ðŸ™„|ðŸ™…|'
        r'ðŸ™†|ðŸ™‡|ðŸ™ˆ|ðŸ™‰|ðŸ™Š|ðŸ™‹|ðŸ™Œ|ðŸ™|ðŸ™Ž|ðŸ™|ðŸ¤”|ðŸ¤—|ðŸ¤|ðŸ¤‘|ðŸ¤“|ðŸ¤•|ðŸ¤ |ðŸ¤¡|ðŸ¤¢|ðŸ¤£|ðŸ¤¤|ðŸ¤¥|ðŸ¤§|ðŸ¤¨|ðŸ¤©|ðŸ¤ª|'
        r'ðŸ¤«|ðŸ¤¬|ðŸ¤­|ðŸ¤®|ðŸ¤¯|ðŸ¤°|ðŸ¤±|ðŸ¤²|ðŸ¤³|ðŸ¤´|ðŸ¤µ|ðŸ¤¶|ðŸ¤·|ðŸ¤¸|ðŸ¤¹|ðŸ¤º|ðŸ¤¼|ðŸ¤½|ðŸ¤¾|ðŸ¤¿|ðŸ¥€|ðŸ¥|ðŸ¥‚|ðŸ¥ƒ|ðŸ¥„|ðŸ¥…|'
        r'ðŸ¥‡|ðŸ¥ˆ|ðŸ¥‰|ðŸ¥Š|ðŸ¥‹|ðŸ¥Œ|ðŸ¥|ðŸ¥Ž|ðŸ¥|ðŸ¥|ðŸ¥‘|ðŸ¥’|ðŸ¥“|ðŸ¥”|ðŸ¥•|ðŸ¥–|ðŸ¥—|ðŸ¥˜|ðŸ¥™|ðŸ¥š|ðŸ¥›|ðŸ¥œ|ðŸ¥|ðŸ¥ž|ðŸ¥Ÿ|ðŸ¥ |'
        r'ðŸ¥¡|ðŸ¥¢|ðŸ¥£|ðŸ¥¤|ðŸ¥¥|ðŸ¥¦|ðŸ¥§|ðŸ¥¨|ðŸ¥©|ðŸ¥ª|ðŸ¥«|ðŸ¥¬|ðŸ¥­|ðŸ¥®|ðŸ¥¯|ðŸ¥°|ðŸ¥±|ðŸ¥²|ðŸ¥³|ðŸ¥´|ðŸ¥µ|ðŸ¥¶|ðŸ¥·|ðŸ¥¸|ðŸ¥º|ðŸ¥»|'
        r'ðŸ¥¼|ðŸ¥½|ðŸ¥¾|ðŸ¥¿|ðŸ¤–|ðŸ¤ |ðŸ¤¤|ðŸ¤©|ðŸ¤¯|ðŸ¤ª|ðŸ¥³|ðŸ¤ |ðŸ¤¯|ðŸ¤¨|ðŸ¤|ðŸ¤¬|ðŸ¤”|ðŸ¤•|ðŸ¤“|ðŸ¤—|ðŸ˜…|ðŸ˜†|ðŸ˜‡|ðŸ˜ˆ|ðŸ˜‰|ðŸ˜Š|'
        r'ðŸ˜‹|ðŸ˜Œ|ðŸ˜|ðŸ˜Ž|ðŸ˜|ðŸ˜|ðŸ˜‘|ðŸ˜’|ðŸ˜“|ðŸ˜”|ðŸ˜•|ðŸ˜–|ðŸ˜—|ðŸ˜˜|ðŸ˜™|ðŸ˜š|ðŸ˜›|ðŸ˜œ|ðŸ˜|ðŸ˜ž|ðŸ˜Ÿ|ðŸ˜ |ðŸ˜¡|ðŸ˜¢|ðŸ˜£|ðŸ˜¤|ðŸ˜¥|ðŸ˜¦|'
        r'ðŸ˜§|ðŸ˜¨|ðŸ˜©|ðŸ˜ª|ðŸ˜«|ðŸ˜¬|ðŸ˜­|ðŸ˜®|ðŸ˜¯|ðŸ˜°|ðŸ˜±|ðŸ˜²|ðŸ˜³|ðŸ˜´|ðŸ˜µ|ðŸ˜¶|ðŸ˜·|ðŸ˜¸|ðŸ˜¹|ðŸ˜º|ðŸ˜»|ðŸ˜¼|ðŸ˜½|ðŸ˜¾|ðŸ˜¿|ðŸ™€|ðŸ™|ðŸ™‚|'
        r'ðŸ™ƒ|ðŸ™„|ðŸ™…|ðŸ™†|ðŸ™‡|ðŸ™ˆ|ðŸ™‰|ðŸ™Š|ðŸ™‹|ðŸ™Œ|ðŸ™|ðŸ™Ž|ðŸ™|ðŸ¤”|ðŸ¤—|ðŸ¤|ðŸ¤‘|ðŸ¤“|ðŸ¤•|ðŸ¤ |ðŸ¤¡|ðŸ¤¢|ðŸ¤£|ðŸ¤¤|ðŸ¤¥|ðŸ¤§|'
        r'ðŸ¤¨|ðŸ¤©|ðŸ¤ª|ðŸ¤«|ðŸ¤¬|ðŸ¤­|ðŸ¤®|ðŸ¤¯|ðŸ¤°|ðŸ¤±|ðŸ¤²|ðŸ¤³|ðŸ¤´|ðŸ¤µ|ðŸ¤¶|ðŸ¤·|ðŸ¤¸|ðŸ¤¹|ðŸ¤º|ðŸ¤¼|ðŸ¤½|ðŸ¤¾|ðŸ¤¿|ðŸ¥€|ðŸ¥|ðŸ¥‚|'
        r'ðŸ¥ƒ|ðŸ¥„|ðŸ¥…|ðŸ¥‡|ðŸ¥ˆ|ðŸ¥‰|ðŸ¥Š|ðŸ¥‹|ðŸ¥Œ|ðŸ¥|ðŸ¥Ž|ðŸ¥|ðŸ¥|ðŸ¥‘|ðŸ¥’|ðŸ¥“|ðŸ¥”|ðŸ¥•|ðŸ¥–|ðŸ¥—|ðŸ¥˜|ðŸ¥™|ðŸ¥š|ðŸ¥›|ðŸ¥œ|ðŸ¥|'
        r'ðŸ¥ž|ðŸ¥Ÿ|ðŸ¥ |ðŸ¥¡|ðŸ¥¢|ðŸ¥£|ðŸ¥¤|ðŸ¥¥|ðŸ¥¦|ðŸ¥§|ðŸ¥¨|ðŸ¥©|ðŸ¥ª|ðŸ¥«|ðŸ¥¬|ðŸ¥­|ðŸ¥®|ðŸ¥¯|ðŸ¥°|ðŸ¥±|ðŸ¥²|ðŸ¥³|ðŸ¥´|ðŸ¥µ|ðŸ¥¶|ðŸ¥·|'
        r'ðŸ¥¸|ðŸ¥º|ðŸ¥»|ðŸ¥¼|ðŸ¥½|ðŸ¥¾|ðŸ¥¿|ðŸ¤–|ðŸ¤ |ðŸ¤¤|ðŸ¤©|ðŸ¤¯|ðŸ¤ª|ðŸ¥³|ðŸ¤ |ðŸ¤¯|ðŸ¤¨|ðŸ¤|ðŸ¤¬|ðŸ¤”|ðŸ¤•|ðŸ¤“|ðŸ¤—|ðŸ˜…|ðŸ˜†|ðŸ˜‡|'
        r'ðŸ˜ˆ|ðŸ˜‰|ðŸ˜Š|ðŸ˜‹|ðŸ˜Œ|ðŸ˜|ðŸ˜Ž|ðŸ˜|ðŸ˜|ðŸ˜‘|ðŸ˜’|ðŸ˜“|ðŸ˜”|ðŸ˜•|ðŸ˜–|ðŸ˜—|ðŸ˜˜|ðŸ˜™|ðŸ˜š|ðŸ˜›|ðŸ˜œ|ðŸ˜|ðŸ˜ž|ðŸ˜Ÿ|ðŸ˜ |ðŸ˜¡|ðŸ˜¢|ðŸ˜£|ðŸ˜¤|'
        r'ðŸ˜¥|ðŸ˜¦|ðŸ˜§|ðŸ˜¨|ðŸ˜©|ðŸ˜ª|ðŸ˜«|ðŸ˜¬|ðŸ˜­|ðŸ˜®|ðŸ˜¯|ðŸ˜°|ðŸ˜±|ðŸ˜²|ðŸ˜³|ðŸ˜´|ðŸ˜µ|ðŸ˜¶|ðŸ˜·|ðŸ˜¸|ðŸ˜¹|ðŸ˜º|ðŸ˜»|ðŸ˜¼|ðŸ˜½|ðŸ˜¾|ðŸ˜¿|ðŸ™€|'
        r'ðŸ™|ðŸ™‚|ðŸ™ƒ|ðŸ™„|ðŸ™…|ðŸ™†|ðŸ™‡|ðŸ™ˆ|ðŸ™‰|ðŸ™Š|ðŸ™‹|ðŸ™Œ|ðŸ™|ðŸ™Ž|ðŸ™|ðŸ¤”|ðŸ¤—|ðŸ¤|ðŸ¤‘|ðŸ¤“|ðŸ¤•|ðŸ¤ |ðŸ¤¡|ðŸ¤¢|ðŸ¤£|ðŸ¤¤|'
        r'ðŸ¤¥|ðŸ¤§|ðŸ¤¨|ðŸ¤©|ðŸ¤ª|ðŸ¤«|ðŸ¤¬|ðŸ¤­|ðŸ¤®|ðŸ¤¯|ðŸ¤°|ðŸ¤±|ðŸ¤²|ðŸ¤³|ðŸ¤´|ðŸ¤µ|ðŸ¤¶|ðŸ¤·|ðŸ¤¸|ðŸ¤¹|ðŸ¤º|ðŸ¤¼|ðŸ¤½|ðŸ¤¾|ðŸ¤¿|'
        r'ðŸ¥€|ðŸ¥|ðŸ¥‚|ðŸ¥ƒ|ðŸ¥„|ðŸ¥…|ðŸ¥‡|ðŸ¥ˆ|ðŸ¥‰|ðŸ¥Š|ðŸ¥‹|ðŸ¥Œ|ðŸ¥|ðŸ¥Ž|ðŸ¥|ðŸ¥|ðŸ¥‘|ðŸ¥’|ðŸ¥“|ðŸ¥”|ðŸ¥•|ðŸ¥–|ðŸ¥—|ðŸ¥˜|ðŸ¥™|ðŸ¥š|'
        r'ðŸ¥›|ðŸ¥œ|ðŸ¥|ðŸ¥ž|ðŸ¥Ÿ|ðŸ¥ |ðŸ¥¡|ðŸ¥¢|ðŸ¥£|ðŸ¥¤|ðŸ¥¥|ðŸ¥¦|ðŸ¥§|ðŸ¥¨|ðŸ¥©|ðŸ¥ª|ðŸ¥«|ðŸ¥¬|ðŸ¥­|ðŸ¥®|ðŸ¥¯|ðŸ¥°|ðŸ¥±|ðŸ¥²|ðŸ¥³|ðŸ¥´|'
        r'ðŸ¥µ|ðŸ¥¶|ðŸ¥·|ðŸ¥¸|ðŸ¥º|ðŸ¥»|ðŸ¥¼|ðŸ¥½|ðŸ¥¾|ðŸ¥¿|ðŸ¤–|ðŸ¤ |ðŸ¤¤|ðŸ¤©|ðŸ¤¯|ðŸ¤ª|ðŸ¥³|ðŸ¤ |ðŸ¤¯|ðŸ¤¨|ðŸ¤|ðŸ¤¬|ðŸ¤”|ðŸ¤•|ðŸ¤“|ðŸ¤—|'
        r'ðŸ˜…|ðŸ˜†|ðŸ˜‡|ðŸ˜ˆ|ðŸ˜‰|ðŸ˜Š|ðŸ˜‹|ðŸ˜Œ|ðŸ˜|ðŸ˜Ž|ðŸ˜|ðŸ˜|ðŸ˜‘|ðŸ˜’|ðŸ˜“|ðŸ˜”|ðŸ˜•|ðŸ˜–|ðŸ˜—|ðŸ˜˜|ðŸ˜™|ðŸ˜š|ðŸ˜›|ðŸ˜œ|ðŸ˜|ðŸ˜ž|ðŸ˜Ÿ|ðŸ˜ |'
        r'ðŸ˜¡|ðŸ˜¢|ðŸ˜£|ðŸ˜¤|ðŸ˜¥|ðŸ˜¦|ðŸ˜§|ðŸ˜¨|ðŸ˜©|ðŸ˜ª|ðŸ˜«|ðŸ˜¬|ðŸ˜­|ðŸ˜®|ðŸ˜¯|ðŸ˜°|ðŸ˜±|ðŸ˜²|ðŸ˜³|ðŸ˜´|ðŸ˜µ|ðŸ˜¶|ðŸ˜·|ðŸ˜¸|ðŸ˜¹|ðŸ˜º|ðŸ˜»|ðŸ˜¼|'
        r'ðŸ˜½|ðŸ˜¾|ðŸ˜¿|ðŸ™€|ðŸ™|ðŸ™‚|ðŸ™ƒ|ðŸ™„|ðŸ™…|ðŸ™†|ðŸ™‡|ðŸ™ˆ|ðŸ™‰|ðŸ™Š|ðŸ™‹|ðŸ™Œ|ðŸ™|ðŸ™Ž|ðŸ™|ðŸ¤”|ðŸ¤—|ðŸ¤|ðŸ¤‘|ðŸ¤“|ðŸ¤•|ðŸ¤ |'
        r'ðŸ¤¡|ðŸ¤¢|ðŸ¤£|ðŸ¤¤|ðŸ¤¥|ðŸ¤§|ðŸ¤¨|ðŸ¤©|ðŸ¤ª|ðŸ¤«|ðŸ¤¬|ðŸ¤­|ðŸ¤®|ðŸ¤¯|ðŸ¤°|ðŸ¤±|ðŸ¤²|ðŸ¤³|ðŸ¤´|ðŸ¤µ|ðŸ¤¶|ðŸ¤·|ðŸ¤¸|ðŸ¤¹|ðŸ¤º|ðŸ¤¼|'
        r'ðŸ¤½|ðŸ¤¾|ðŸ¤¿|ðŸ¥€|ðŸ¥|ðŸ¥‚|ðŸ¥ƒ|ðŸ¥„|ðŸ¥…|ðŸ¥‡|ðŸ¥ˆ|ðŸ¥‰|ðŸ¥Š|ðŸ¥‹|ðŸ¥Œ|ðŸ¥|ðŸ¥Ž|ðŸ¥|ðŸ¥|ðŸ¥‘|ðŸ¥’|ðŸ¥“|ðŸ¥”|ðŸ¥•|ðŸ¥–|ðŸ¥—|'
        r'ðŸ¥˜|ðŸ¥™|ðŸ¥š|ðŸ¥›|ðŸ¥œ|ðŸ¥|ðŸ¥ž|ðŸ¥Ÿ'
    )
    text = re.sub(emoticons_pattern, '', text)

    text = text.replace('\n', ' ')
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = text.strip(' ')
    return text

def casefoldingText(text):
    text = text.lower()
    return text

def tokenizingText(text):
    text = word_tokenize(text)
    return text

def filteringText(text):
    listStopwords = set(stopwords.words('indonesian'))
    filtered = [txt for txt in text if txt not in listStopwords]
    return filtered

def stemmingText(text):
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    text = [stemmer.stem(word) for word in text]
    return text

def toSentence(list_words):
    sentence = ' '.join(word for word in list_words)
    return sentence

tweets['text_clean'] = tweets['full_text'].apply(cleaningText)
tweets['text_clean'] = tweets['text_clean'].apply(casefoldingText)
tweets.drop(['full_text'], axis=1, inplace=True)

tweets['text_clean'] = tweets['text_clean'].apply(replace_alay)

tweets['text_preprocessed'] = tweets['text_clean'].apply(tokenizingText)
tweets['text_preprocessed'] = tweets['text_preprocessed'].apply(filteringText)
tweets['text_preprocessed'] = tweets['text_preprocessed'].apply(stemmingText)
tweets.drop_duplicates(subset='text_clean', inplace=True)

tweets.to_csv(r'data_clean.csv', index=False, header=True, index_label=None)

tweets = pd.read_csv('/content/data_clean_israel.csv')

for i, text in enumerate(tweets['text_preprocessed']):
    tweets['text_preprocessed'][i] = tweets['text_preprocessed'][i].replace("'", "")\
                                            .replace(',','').replace(']','').replace('[','')
    list_words=[]
    for word in tweets['text_preprocessed'][i].split():
        list_words.append(word)

    tweets['text_preprocessed'][i] = list_words

tweets

lexicon = dict()
import csv

with open('/content/modified_full_lexiconn.csv', 'r') as csvfile:
    reader = csv.reader(csvfile, delimiter=',')
    next(reader)
    for row in reader:
        lexicon[row[0]] = int(row[1])


def sentiment_analysis_lexicon_indonesia(text):
    weight = 0
    for word in text:
        if word in lexicon:
            weight += lexicon[word]

    polarity = 'positive' if weight > 0 else ('negative' if weight < 0 else 'neutral')
    return weight, polarity

results = tweets['text_preprocessed'].apply(sentiment_analysis_lexicon_indonesia)
results = list(zip(*results))
tweets['polarity_score'] = results[0]
tweets['polarity'] = results[1]
print(tweets['polarity'].value_counts())

tweets.to_csv(r'sentimen.csv', index = False, header = True,index_label=None)

fig, ax = plt.subplots(figsize = (6, 6))
sizes = [count for count in tweets['polarity'].value_counts()]
labels = list(tweets['polarity'].value_counts().index)
explode = (0.1, 0, 0)
ax.pie(x = sizes, labels = labels, autopct = '%1.1f%%', explode = explode, textprops={'fontsize': 14})
ax.set_title('Sentiment Polarity on Tweets Data', fontsize = 16, pad = 20)
plt.show()

pd.set_option('display.max_colwidth', 3000)
positive_tweets = tweets[tweets['polarity'] == 'positive']
positive_tweets = positive_tweets[['text_clean', 'polarity_score', 'polarity']].sort_values(by = 'polarity_score', ascending=False).reset_index(drop = True)
positive_tweets.index += 1
positive_tweets[0:5]

pd.set_option('display.max_colwidth', 3000)
negative_tweets = tweets[tweets['polarity'] == 'negative']
negative_tweets = negative_tweets[['text_clean', 'polarity_score', 'polarity']].sort_values(by = 'polarity_score', ascending=True)[0:10].reset_index(drop = True)
negative_tweets.index += 1
negative_tweets[0:5]

list_words=''
for tweet in tweets['text_preprocessed']:
    for word in tweet:
        list_words += ' '+(word)

wordcloud = WordCloud(width = 600, height = 400, background_color = 'black', min_font_size = 10).generate(list_words)
fig, ax = plt.subplots(figsize = (8, 6))
ax.set_title('Word Cloud of Tweets Data', fontsize = 18)
ax.grid(False)
ax.imshow((wordcloud))
fig.tight_layout(pad=0)
ax.axis('off')
plt.show()

def words_with_sentiment(text):
    positive_words = []
    negative_words = []
    for word in text:
        score_pos = 0
        score_neg = 0
        if word in lexicon:
            score_pos = lexicon[word]

        if score_pos > 0:
            positive_words.append(word)
        elif score_pos < 0:
            negative_words.append(word)

    return positive_words, negative_words

sentiment_words = tweets['text_preprocessed'].apply(words_with_sentiment)
sentiment_words = list(zip(*sentiment_words))
positive_words = sentiment_words[0]
negative_words = sentiment_words[1]

fig, ax = plt.subplots(1, 2,figsize = (12, 10))
list_words_postive=''
for row_word in positive_words:
    for word in row_word:
        list_words_postive += ' '+(word)
wordcloud_positive = WordCloud(width = 800, height = 600, background_color = 'black', colormap = 'Greens'
                               , min_font_size = 10).generate(list_words_postive)
ax[0].set_title('Word Cloud of Positive Words on Tweets Data)', fontsize = 14)
ax[0].grid(False)
ax[0].imshow((wordcloud_positive))
fig.tight_layout(pad=0)
ax[0].axis('off')

list_words_negative=''
for row_word in negative_words:
    for word in row_word:
        list_words_negative += ' '+(word)
wordcloud_negative = WordCloud(width = 800, height = 600, background_color = 'black', colormap = 'Reds'
                               , min_font_size = 10).generate(list_words_negative)
ax[1].set_title('Word Cloud of Negative Words on Tweets Data)', fontsize = 14)
ax[1].grid(False)
ax[1].imshow((wordcloud_negative))
fig.tight_layout(pad=0)
ax[1].axis('off')

plt.show()

data = pd.read_csv('/content/sentimen israel.csv')
data.head()

X_train, X_test, y_train, y_test = train_test_split(data['text_preprocessed'], data['polarity'], test_size=0.2, random_state=0)
tfidf_vectorizer = TfidfVectorizer(max_features=2000)

X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)
feature_names = tfidf_vectorizer.get_feature_names_out()

naive_bayes = MultinomialNB()
naive_bayes.fit(X_train_tfidf, y_train)
y_pred = naive_bayes.predict(X_test_tfidf)

accuracy = accuracy_score(y_test, y_pred)
cv_scores = cross_val_score(RandomForestClassifier(random_state=0), X_train_tfidf, y_train, cv=5)
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", np.mean(cv_scores))
print(f'Accuracy: {accuracy:.2f}')
print('\nClassification Report:')
print(classification_report(y_test, y_pred))
print('\nConfusion Matrix:')
print(confusion_matrix(y_test, y_pred))

X_train, X_test, y_train, y_test = train_test_split(data['text_preprocessed'], data['polarity'], test_size=0.2, random_state=0)
bow_vectorizer = CountVectorizer(max_features=1000)
X_train_bow = bow_vectorizer.fit_transform(X_train)
X_test_bow = bow_vectorizer.transform(X_test)

random_forest = RandomForestClassifier(n_estimators=100, random_state=0)
random_forest.fit(X_train_bow, y_train)

y_pred = random_forest.predict(X_test_bow)

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
print('\nClassification Report:')
print(classification_report(y_test, y_pred))
print('\nConfusion Matrix:')
print(confusion_matrix(y_test, y_pred))