# -*- coding: utf-8 -*-
"""Israel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J9I4K5P7WLT4an4lO6YsEGTfkHYSp7bE

### Import Library
"""

pip install Sastrawi

import nltk
from nltk.tokenize import word_tokenize

# Download the NLTK data for tokenization
nltk.download('punkt')

def tokenizingText(text):  # Tokenizing or splitting a string, text into a list of tokens
    text = word_tokenize(text)
    return text

import pandas as pd
pd.options.mode.chained_assignment = None
import numpy as np
seed = 0
np.random.seed(seed)
import matplotlib.pyplot as plt
import seaborn as sns

import datetime as dt
import re
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from wordcloud import WordCloud

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score

"""# Prepare untuk fitur yang digunakan

Tokenize
"""

import nltk
from nltk.tokenize import word_tokenize

nltk.download('stopwords')
def tokenizingText(text):
    text = word_tokenize(text)
    return text

import csv
input_file_path = '/content/israel no buzzer.csv'
output_file_path = 'rawel.csv'

try:
    with open(input_file_path, 'r') as input_file, open(output_file_path, 'w', newline='') as output_file:
        pembaca = csv.reader(input_file)
        penulis = csv.writer(output_file)
        header = next(pembaca)
        jumlah_kolom = len(header)
        penulis.writerow(["full_text"])

        for nomor_baris, baris in enumerate(pembaca, start=2):
            kolom_gabungan = ','.join(baris)
            penulis.writerow([kolom_gabungan])

    print(f"File CSV yang sudah diperbaiki disimpan di: {output_file_path}")

except Exception as e:
    print(f"Terjadi kesalahan: {e}")

tweets_data = pd.read_csv('/content/rawel.csv')
tweets = tweets_data[['full_text']]
tweets

import pandas as pd
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
import string

alay_dict = pd.read_csv('/content/singkatan-lib.csv', header=None, names=['awal', 'replace'], index_col='awal')

def replace_alay(text):
    for original, replacement in alay_dict.iterrows():
        text = re.sub(r'\b{}\b'.format(re.escape(original)), replacement['replace'], text)
    return text

def cleaningText(text):
    text = re.sub(r'@[A-Za-z0-9]+', '', text)
    text = re.sub(r'#[A-Za-z0-9]+', '', text)
    text = re.sub(r'RT[\s]', '', text)
    text = re.sub(r"http\S+", '', text)
    text = re.sub(r'[0-9]+', '', text)
    emoticons_pattern = (
        r'😊|😀|😁|😂|🤣|😃|😄|😅|😆|😇|😉|😊|🙂|🙃|😋|😌|😍|😎|😏|😐|😑|😒|😓|😔|😕|😖|'
        r'😗|😘|😙|😚|😛|😜|😝|😞|😟|😠|😡|😢|😣|😤|😥|😦|😧|😨|😩|😪|😫|😬|😭|😮|😯|😰|😱|😲|'
        r'😳|😴|😵|😶|😷|😸|😹|😺|😻|😼|😽|😾|😿|🙀|🙁|🙂|🙃|🙄|🙅|🙆|🙇|🙈|🙉|🙊|🙋|🙌|🙍|'
        r'🙎|🙏|😐|😑|😒|😓|😔|😕|😖|😗|😘|😙|😚|😛|😜|😝|😞|😟|😠|😡|😢|😣|😤|😥|😦|😧|😨|😩|'
        r'😪|😫|😬|😭|😮|😯|😰|😱|😲|😳|😴|😵|😶|😷|😸|😹|😺|😻|😼|😽|😾|😿|🙀|🙁|🙂|🙃|🙄|🙅|'
        r'🙆|🙇|🙈|🙉|🙊|🙋|🙌|🙍|🙎|🙏|🤔|🤗|🤐|🤑|🤓|🤕|🤠|🤡|🤢|🤣|🤤|🤥|🤧|🤨|🤩|🤪|'
        r'🤫|🤬|🤭|🤮|🤯|🤰|🤱|🤲|🤳|🤴|🤵|🤶|🤷|🤸|🤹|🤺|🤼|🤽|🤾|🤿|🥀|🥁|🥂|🥃|🥄|🥅|'
        r'🥇|🥈|🥉|🥊|🥋|🥌|🥍|🥎|🥏|🥐|🥑|🥒|🥓|🥔|🥕|🥖|🥗|🥘|🥙|🥚|🥛|🥜|🥝|🥞|🥟|🥠|'
        r'🥡|🥢|🥣|🥤|🥥|🥦|🥧|🥨|🥩|🥪|🥫|🥬|🥭|🥮|🥯|🥰|🥱|🥲|🥳|🥴|🥵|🥶|🥷|🥸|🥺|🥻|'
        r'🥼|🥽|🥾|🥿|🤖|🤠|🤤|🤩|🤯|🤪|🥳|🤠|🤯|🤨|🤐|🤬|🤔|🤕|🤓|🤗|😅|😆|😇|😈|😉|😊|'
        r'😋|😌|😍|😎|😏|😐|😑|😒|😓|😔|😕|😖|😗|😘|😙|😚|😛|😜|😝|😞|😟|😠|😡|😢|😣|😤|😥|😦|'
        r'😧|😨|😩|😪|😫|😬|😭|😮|😯|😰|😱|😲|😳|😴|😵|😶|😷|😸|😹|😺|😻|😼|😽|😾|😿|🙀|🙁|🙂|'
        r'🙃|🙄|🙅|🙆|🙇|🙈|🙉|🙊|🙋|🙌|🙍|🙎|🙏|🤔|🤗|🤐|🤑|🤓|🤕|🤠|🤡|🤢|🤣|🤤|🤥|🤧|'
        r'🤨|🤩|🤪|🤫|🤬|🤭|🤮|🤯|🤰|🤱|🤲|🤳|🤴|🤵|🤶|🤷|🤸|🤹|🤺|🤼|🤽|🤾|🤿|🥀|🥁|🥂|'
        r'🥃|🥄|🥅|🥇|🥈|🥉|🥊|🥋|🥌|🥍|🥎|🥏|🥐|🥑|🥒|🥓|🥔|🥕|🥖|🥗|🥘|🥙|🥚|🥛|🥜|🥝|'
        r'🥞|🥟|🥠|🥡|🥢|🥣|🥤|🥥|🥦|🥧|🥨|🥩|🥪|🥫|🥬|🥭|🥮|🥯|🥰|🥱|🥲|🥳|🥴|🥵|🥶|🥷|'
        r'🥸|🥺|🥻|🥼|🥽|🥾|🥿|🤖|🤠|🤤|🤩|🤯|🤪|🥳|🤠|🤯|🤨|🤐|🤬|🤔|🤕|🤓|🤗|😅|😆|😇|'
        r'😈|😉|😊|😋|😌|😍|😎|😏|😐|😑|😒|😓|😔|😕|😖|😗|😘|😙|😚|😛|😜|😝|😞|😟|😠|😡|😢|😣|😤|'
        r'😥|😦|😧|😨|😩|😪|😫|😬|😭|😮|😯|😰|😱|😲|😳|😴|😵|😶|😷|😸|😹|😺|😻|😼|😽|😾|😿|🙀|'
        r'🙁|🙂|🙃|🙄|🙅|🙆|🙇|🙈|🙉|🙊|🙋|🙌|🙍|🙎|🙏|🤔|🤗|🤐|🤑|🤓|🤕|🤠|🤡|🤢|🤣|🤤|'
        r'🤥|🤧|🤨|🤩|🤪|🤫|🤬|🤭|🤮|🤯|🤰|🤱|🤲|🤳|🤴|🤵|🤶|🤷|🤸|🤹|🤺|🤼|🤽|🤾|🤿|'
        r'🥀|🥁|🥂|🥃|🥄|🥅|🥇|🥈|🥉|🥊|🥋|🥌|🥍|🥎|🥏|🥐|🥑|🥒|🥓|🥔|🥕|🥖|🥗|🥘|🥙|🥚|'
        r'🥛|🥜|🥝|🥞|🥟|🥠|🥡|🥢|🥣|🥤|🥥|🥦|🥧|🥨|🥩|🥪|🥫|🥬|🥭|🥮|🥯|🥰|🥱|🥲|🥳|🥴|'
        r'🥵|🥶|🥷|🥸|🥺|🥻|🥼|🥽|🥾|🥿|🤖|🤠|🤤|🤩|🤯|🤪|🥳|🤠|🤯|🤨|🤐|🤬|🤔|🤕|🤓|🤗|'
        r'😅|😆|😇|😈|😉|😊|😋|😌|😍|😎|😏|😐|😑|😒|😓|😔|😕|😖|😗|😘|😙|😚|😛|😜|😝|😞|😟|😠|'
        r'😡|😢|😣|😤|😥|😦|😧|😨|😩|😪|😫|😬|😭|😮|😯|😰|😱|😲|😳|😴|😵|😶|😷|😸|😹|😺|😻|😼|'
        r'😽|😾|😿|🙀|🙁|🙂|🙃|🙄|🙅|🙆|🙇|🙈|🙉|🙊|🙋|🙌|🙍|🙎|🙏|🤔|🤗|🤐|🤑|🤓|🤕|🤠|'
        r'🤡|🤢|🤣|🤤|🤥|🤧|🤨|🤩|🤪|🤫|🤬|🤭|🤮|🤯|🤰|🤱|🤲|🤳|🤴|🤵|🤶|🤷|🤸|🤹|🤺|🤼|'
        r'🤽|🤾|🤿|🥀|🥁|🥂|🥃|🥄|🥅|🥇|🥈|🥉|🥊|🥋|🥌|🥍|🥎|🥏|🥐|🥑|🥒|🥓|🥔|🥕|🥖|🥗|'
        r'🥘|🥙|🥚|🥛|🥜|🥝|🥞|🥟'
    )
    text = re.sub(emoticons_pattern, '', text)

    text = text.replace('\n', ' ')
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = text.strip(' ')
    return text

def casefoldingText(text):
    text = text.lower()
    return text

def tokenizingText(text):
    text = word_tokenize(text)
    return text

def filteringText(text):
    listStopwords = set(stopwords.words('indonesian'))
    filtered = [txt for txt in text if txt not in listStopwords]
    return filtered

def stemmingText(text):
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    text = [stemmer.stem(word) for word in text]
    return text

def toSentence(list_words):
    sentence = ' '.join(word for word in list_words)
    return sentence

tweets['text_clean'] = tweets['full_text'].apply(cleaningText)
tweets['text_clean'] = tweets['text_clean'].apply(casefoldingText)
tweets.drop(['full_text'], axis=1, inplace=True)

tweets['text_clean'] = tweets['text_clean'].apply(replace_alay)

tweets['text_preprocessed'] = tweets['text_clean'].apply(tokenizingText)
tweets['text_preprocessed'] = tweets['text_preprocessed'].apply(filteringText)
tweets['text_preprocessed'] = tweets['text_preprocessed'].apply(stemmingText)
tweets.drop_duplicates(subset='text_clean', inplace=True)

tweets.to_csv(r'data_clean.csv', index=False, header=True, index_label=None)

tweets = pd.read_csv('/content/data_clean_israel.csv')

for i, text in enumerate(tweets['text_preprocessed']):
    tweets['text_preprocessed'][i] = tweets['text_preprocessed'][i].replace("'", "")\
                                            .replace(',','').replace(']','').replace('[','')
    list_words=[]
    for word in tweets['text_preprocessed'][i].split():
        list_words.append(word)

    tweets['text_preprocessed'][i] = list_words

tweets

lexicon = dict()
import csv

with open('/content/modified_full_lexiconn.csv', 'r') as csvfile:
    reader = csv.reader(csvfile, delimiter=',')
    next(reader)
    for row in reader:
        lexicon[row[0]] = int(row[1])


def sentiment_analysis_lexicon_indonesia(text):
    weight = 0
    for word in text:
        if word in lexicon:
            weight += lexicon[word]

    polarity = 'positive' if weight > 0 else ('negative' if weight < 0 else 'neutral')
    return weight, polarity

results = tweets['text_preprocessed'].apply(sentiment_analysis_lexicon_indonesia)
results = list(zip(*results))
tweets['polarity_score'] = results[0]
tweets['polarity'] = results[1]
print(tweets['polarity'].value_counts())

tweets.to_csv(r'sentimen.csv', index = False, header = True,index_label=None)

fig, ax = plt.subplots(figsize = (6, 6))
sizes = [count for count in tweets['polarity'].value_counts()]
labels = list(tweets['polarity'].value_counts().index)
explode = (0.1, 0, 0)
ax.pie(x = sizes, labels = labels, autopct = '%1.1f%%', explode = explode, textprops={'fontsize': 14})
ax.set_title('Sentiment Polarity on Tweets Data', fontsize = 16, pad = 20)
plt.show()

pd.set_option('display.max_colwidth', 3000)
positive_tweets = tweets[tweets['polarity'] == 'positive']
positive_tweets = positive_tweets[['text_clean', 'polarity_score', 'polarity']].sort_values(by = 'polarity_score', ascending=False).reset_index(drop = True)
positive_tweets.index += 1
positive_tweets[0:5]

pd.set_option('display.max_colwidth', 3000)
negative_tweets = tweets[tweets['polarity'] == 'negative']
negative_tweets = negative_tweets[['text_clean', 'polarity_score', 'polarity']].sort_values(by = 'polarity_score', ascending=True)[0:10].reset_index(drop = True)
negative_tweets.index += 1
negative_tweets[0:5]

list_words=''
for tweet in tweets['text_preprocessed']:
    for word in tweet:
        list_words += ' '+(word)

wordcloud = WordCloud(width = 600, height = 400, background_color = 'black', min_font_size = 10).generate(list_words)
fig, ax = plt.subplots(figsize = (8, 6))
ax.set_title('Word Cloud of Tweets Data', fontsize = 18)
ax.grid(False)
ax.imshow((wordcloud))
fig.tight_layout(pad=0)
ax.axis('off')
plt.show()

def words_with_sentiment(text):
    positive_words = []
    negative_words = []
    for word in text:
        score_pos = 0
        score_neg = 0
        if word in lexicon:
            score_pos = lexicon[word]

        if score_pos > 0:
            positive_words.append(word)
        elif score_pos < 0:
            negative_words.append(word)

    return positive_words, negative_words

sentiment_words = tweets['text_preprocessed'].apply(words_with_sentiment)
sentiment_words = list(zip(*sentiment_words))
positive_words = sentiment_words[0]
negative_words = sentiment_words[1]

fig, ax = plt.subplots(1, 2,figsize = (12, 10))
list_words_postive=''
for row_word in positive_words:
    for word in row_word:
        list_words_postive += ' '+(word)
wordcloud_positive = WordCloud(width = 800, height = 600, background_color = 'black', colormap = 'Greens'
                               , min_font_size = 10).generate(list_words_postive)
ax[0].set_title('Word Cloud of Positive Words on Tweets Data)', fontsize = 14)
ax[0].grid(False)
ax[0].imshow((wordcloud_positive))
fig.tight_layout(pad=0)
ax[0].axis('off')

list_words_negative=''
for row_word in negative_words:
    for word in row_word:
        list_words_negative += ' '+(word)
wordcloud_negative = WordCloud(width = 800, height = 600, background_color = 'black', colormap = 'Reds'
                               , min_font_size = 10).generate(list_words_negative)
ax[1].set_title('Word Cloud of Negative Words on Tweets Data)', fontsize = 14)
ax[1].grid(False)
ax[1].imshow((wordcloud_negative))
fig.tight_layout(pad=0)
ax[1].axis('off')

plt.show()

data = pd.read_csv('/content/sentimen israel.csv')
data.head()

X_train, X_test, y_train, y_test = train_test_split(data['text_preprocessed'], data['polarity'], test_size=0.2, random_state=0)
tfidf_vectorizer = TfidfVectorizer(max_features=2000)

X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)
feature_names = tfidf_vectorizer.get_feature_names_out()

naive_bayes = MultinomialNB()
naive_bayes.fit(X_train_tfidf, y_train)
y_pred = naive_bayes.predict(X_test_tfidf)

accuracy = accuracy_score(y_test, y_pred)
cv_scores = cross_val_score(RandomForestClassifier(random_state=0), X_train_tfidf, y_train, cv=5)
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", np.mean(cv_scores))
print(f'Accuracy: {accuracy:.2f}')
print('\nClassification Report:')
print(classification_report(y_test, y_pred))
print('\nConfusion Matrix:')
print(confusion_matrix(y_test, y_pred))

X_train, X_test, y_train, y_test = train_test_split(data['text_preprocessed'], data['polarity'], test_size=0.2, random_state=0)
bow_vectorizer = CountVectorizer(max_features=1000)
X_train_bow = bow_vectorizer.fit_transform(X_train)
X_test_bow = bow_vectorizer.transform(X_test)

random_forest = RandomForestClassifier(n_estimators=100, random_state=0)
random_forest.fit(X_train_bow, y_train)

y_pred = random_forest.predict(X_test_bow)

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
print('\nClassification Report:')
print(classification_report(y_test, y_pred))
print('\nConfusion Matrix:')
print(confusion_matrix(y_test, y_pred))